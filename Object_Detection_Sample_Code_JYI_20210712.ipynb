{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_Detection_Sample_Code_JYI_20210712",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JAMES-YI/T01_Tensorflow_Tutorials/blob/master/Object_Detection_Sample_Code_JYI_20210712.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98rds-2OU-Rd"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Hub Authors.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\n",
        "Codes originally from: https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\n",
        "\n",
        "Modified by JYI, 07/12/2021"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1c95xMGcU5_Z"
      },
      "source": [
        "#@title Copyright 2020 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1UUX8SUUiMO"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/hub/tutorials/tf2_object_detection\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/hub/examples/colab/tf2_object_detection.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://tfhub.dev/tensorflow/collections/object_detection/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub models</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRImnk_7WOq1"
      },
      "source": [
        "\n",
        "- visit [object detection models in TF]([This](https://tfhub.dev/tensorflow/collections/object_detection/1))\n",
        "- visit [object detection models in tfhub](https://tfhub.dev/s?module-type=image-object-detection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPs64QA1Zdov"
      },
      "source": [
        "# Step: Library installation, imports and setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk4FU-jx9kc3"
      },
      "source": [
        "# This Colab requires TF 2.5. Upgrade tensorflow to 2.5\n",
        "!pip install -U tensorflow>=2.5 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_op5pigPkB8s"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import io\n",
        "import scipy.misc\n",
        "import numpy as np\n",
        "from six import BytesIO\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from six.moves.urllib.request import urlopen\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2CNQV-2-q79"
      },
      "source": [
        "Documentations\n",
        "- pip install, https://pip.pypa.io/en/stable/cli/pip_install/\n",
        "- tf.get_logger().setLevel('ERROR'), control the printing of log information [ref 1](https://stackoverflow.com/questions/38073432/how-to-suppress-verbose-tensorflow-logging)\n",
        "-\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IogyryF2lFBL"
      },
      "source": [
        "# Step: Utilities\n",
        "\n",
        "Run the following cell to create some utils that will be needed later:\n",
        "\n",
        "- Helper method to load an image\n",
        "- Map of Model Name to TF Hub handle\n",
        "- List of tuples with Human Keypoints for the COCO 2017 dataset. This is needed for models with keypoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_pfHxq2Beve"
      },
      "source": [
        "def load_image_into_numpy_array(path):\n",
        "  \"\"\"Load an image from file into a numpy array.\n",
        "\n",
        "  Puts image into numpy array to feed into tensorflow graph.\n",
        "  Note that by convention we put it into a numpy array with shape\n",
        "  (height, width, channels), where channels=3 for RGB.\n",
        "\n",
        "  Args:\n",
        "    path: the file path to the image\n",
        "\n",
        "  Returns:\n",
        "    uint8 numpy array with shape (img_height, img_width, 3)\n",
        "  \"\"\"\n",
        "  image = None\n",
        "  if(path.startswith('http')):\n",
        "    response = urlopen(path)\n",
        "    image_data = response.read()\n",
        "    image_data = BytesIO(image_data)\n",
        "    image = Image.open(image_data)\n",
        "  else:\n",
        "    image_data = tf.io.gfile.GFile(path, 'rb').read()\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "\n",
        "  (im_width, im_height) = image.size\n",
        "  return np.array(image.getdata()).reshape(\n",
        "      (1, im_height, im_width, 3)).astype(np.uint8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAWLRT3YnSMa"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of image loading\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# IMAGES_FOR_TEST = {\n",
        "#   'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
        "#   'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
        "#   # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
        "#   'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
        "#   # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
        "#   'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
        "#   # By Am√©rico Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
        "#   'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
        "#   # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
        "#   'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
        "# }\n",
        "\n",
        "# img_path = IMAGES_FOR_TEST[\"Birds\"]\n",
        "\n",
        "# response = urlopen(img_path)\n",
        "# # print(f\"response: {response}\")\n",
        "# # print(f\"response: {response.read()}\")\n",
        "# # print(f\"response: {type(response.read())}\")\n",
        "\n",
        "# image_data = response.read() # binary data\n",
        "# print(f\"image_data: {image_data}\")\n",
        "\n",
        "# image_data = BytesIO(image_data) # io.Bytesio object\n",
        "# print(f\"image data: {image_data}\")\n",
        "\n",
        "# image = Image.open(image_data) # PIL.JpegImagePlugin.JpegImageFile\n",
        "# print(f\"image: {image}\")\n",
        "# print(f\"image size: {image.size}\")\n",
        "\n",
        "# image = image.getdata() # ImagingCore object\n",
        "# print(f\"image: {image}\")\n",
        "\n",
        "# image = np.array(image) # np.ndarray\n",
        "# print(f\"image: {image}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz3AxDcB_5Rb"
      },
      "source": [
        "Tips\n",
        "- images need to be imported as numpy array to feed into tensorflow graph\n",
        "- urlopen() returns a response object, and the read method of response object can read binary data --> BytesIO() returns an io.BytesIO object --> Image.open() returns a PIL.JpegImagePlugin object --> PIL.JpegImagePlugin object.getdata() returns a ImagingCore object --> np.array turns ImagingCore object into np.ndarray\n",
        "- \n",
        "\n",
        "\n",
        "ToDos\n",
        "- try other models\n",
        "- try other images\n",
        "- read papers about the models\n",
        "- meaning of COCO17_HUMAN_POSE_KEYPOINTS? specifies how the 17 key points are connected\n",
        "- explore all the models in ALL_MODELS\n",
        "\n",
        "Documentations\n",
        "- path.startswith, https://www.tutorialspoint.com/python/string_startswith.htm\n",
        "- urlopenÔºåhttps://docs.python.org/3/library/urllib.request.html\n",
        "- BytesIO, https://docs.python.org/3/library/io.html#io.BytesIO\n",
        "- Image.open, https://pillow.readthedocs.io/en/stable/reference/Image.html\n",
        "- tf.io.gfile.GFile, https://www.tensorflow.org/api_docs/python/tf/io/gfile/GFile\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y9R0Xllefec"
      },
      "source": [
        "# model list, and image list\n",
        "\n",
        "ALL_MODELS = {\n",
        "'CenterNet HourGlass104 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512/1',\n",
        "'CenterNet HourGlass104 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/hourglass_512x512_kpts/1',\n",
        "'CenterNet HourGlass104 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024/1',\n",
        "'CenterNet HourGlass104 Keypoints 1024x1024' : 'https://tfhub.dev/tensorflow/centernet/hourglass_1024x1024_kpts/1',\n",
        "'CenterNet Resnet50 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v1_fpn_512x512_kpts/1',\n",
        "'CenterNet Resnet101 V1 FPN 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet101v1_fpn_512x512/1',\n",
        "'CenterNet Resnet50 V2 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512/1',\n",
        "'CenterNet Resnet50 V2 Keypoints 512x512' : 'https://tfhub.dev/tensorflow/centernet/resnet50v2_512x512_kpts/1',\n",
        "'EfficientDet D0 512x512' : 'https://tfhub.dev/tensorflow/efficientdet/d0/1',\n",
        "'EfficientDet D1 640x640' : 'https://tfhub.dev/tensorflow/efficientdet/d1/1',\n",
        "'EfficientDet D2 768x768' : 'https://tfhub.dev/tensorflow/efficientdet/d2/1',\n",
        "'EfficientDet D3 896x896' : 'https://tfhub.dev/tensorflow/efficientdet/d3/1',\n",
        "'EfficientDet D4 1024x1024' : 'https://tfhub.dev/tensorflow/efficientdet/d4/1',\n",
        "'EfficientDet D5 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d5/1',\n",
        "'EfficientDet D6 1280x1280' : 'https://tfhub.dev/tensorflow/efficientdet/d6/1',\n",
        "'EfficientDet D7 1536x1536' : 'https://tfhub.dev/tensorflow/efficientdet/d7/1',\n",
        "'SSD MobileNet v2 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/2',\n",
        "'SSD MobileNet V1 FPN 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v1/fpn_640x640/1',\n",
        "'SSD MobileNet V2 FPNLite 320x320' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_320x320/1',\n",
        "'SSD MobileNet V2 FPNLite 640x640' : 'https://tfhub.dev/tensorflow/ssd_mobilenet_v2/fpnlite_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_640x640/1',\n",
        "'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https://tfhub.dev/tensorflow/retinanet/resnet50_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_640x640/1',\n",
        "'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https://tfhub.dev/tensorflow/retinanet/resnet101_v1_fpn_1024x1024/1',\n",
        "'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_640x640/1',\n",
        "'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https://tfhub.dev/tensorflow/retinanet/resnet152_v1_fpn_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1',\n",
        "'Faster R-CNN ResNet50 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet50 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet101 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_640x640/1',\n",
        "'Faster R-CNN ResNet101 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet101 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet101_v1_800x1333/1',\n",
        "'Faster R-CNN ResNet152 V1 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_640x640/1',\n",
        "'Faster R-CNN ResNet152 V1 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_1024x1024/1',\n",
        "'Faster R-CNN ResNet152 V1 800x1333' : 'https://tfhub.dev/tensorflow/faster_rcnn/resnet152_v1_800x1333/1',\n",
        "'Faster R-CNN Inception ResNet V2 640x640' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_640x640/1',\n",
        "'Faster R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/faster_rcnn/inception_resnet_v2_1024x1024/1',\n",
        "'Mask R-CNN Inception ResNet V2 1024x1024' : 'https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1'\n",
        "}\n",
        "\n",
        "COCO17_HUMAN_POSE_KEYPOINTS = [(0, 1),\n",
        " (0, 2),\n",
        " (1, 3),\n",
        " (2, 4),\n",
        " (0, 5),\n",
        " (0, 6),\n",
        " (5, 7),\n",
        " (7, 9),\n",
        " (6, 8),\n",
        " (8, 10),\n",
        " (5, 6),\n",
        " (5, 11),\n",
        " (6, 12),\n",
        " (11, 12),\n",
        " (11, 13),\n",
        " (13, 15),\n",
        " (12, 14),\n",
        " (14, 16)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14bNk1gzh0TN"
      },
      "source": [
        "# Step: Visualization tools\n",
        "\n",
        "commonly used utility functions\n",
        "- visualize the images with the proper detected boxes, keypoints and segmentation, we will use the TensorFlow Object Detection API. To install it we will clone the repo.\n",
        "\n",
        "Tips\n",
        "- visit https://github.com/tensorflow/models\n",
        "- file managements in colab, https://neptune.ai/blog/google-colab-dealing-with-files\n",
        "- use line-magic (%) or bash (!) to use shell commands\n",
        "- clone entire github repository to colab environment via git clone; read\n",
        "files as in local machines\n",
        "- access local file systems using python code, from google.colab import files\n",
        "- access google drive from google colab, from google.colab import drive\n",
        "- access google sheets from google colab\n",
        "- access google cloud storage from google colab\n",
        "- access AWS S3 from google colab\n",
        "- access kaggle datasets from google colab\n",
        "- access MySQL databases from google colab\n",
        "\n",
        "ToDos\n",
        "- how to check the directories in colab? use file panel on the left, https://neptune.ai/blog/google-colab-dealing-with-files\n",
        "\n",
        "\n",
        "Documentations\n",
        "- git clone --depth, https://www.perforce.com/blog/vcs/git-beyond-basics-using-shallow-clones "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi28cqGGFWnY"
      },
      "source": [
        "# Clone the tensorflow models repository for the most recent commit\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0HWFLeNZT3a"
      },
      "source": [
        "Tips\n",
        "- after clone all the files from https://github.com/tensorflow/models, we simply use it as common functions. The function dependencies have been set up.\n",
        "- protoc object_detection/protos/*.proto --python_out=. (this command converts the codes into python codes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqaSJfBaDYzq"
      },
      "source": [
        "Documentations\n",
        "- sudo apt install, https://linuxize.com/post/how-to-use-apt-command/\n",
        "- protobuf-compiler, https://developers.google.com/protocol-buffers/docs/overview\n",
        "- protoc, https://developers.google.com/protocol-buffers/docs/proto3\n",
        "- more about protoc see [ref1](https://towardsdatascience.com/how-to-install-tensorflow-2-object-detection-api-on-windows-2eef9b7ae869)\n",
        "- python -m pip install . (this command will install all the listed libraries in .)\n",
        "- cp object_detection/packages/tf2/setup.py . (this command copies all contents in setup.py to .)\n",
        "- more about cp in terminal see [ref 1](https://www.geeksforgeeks.org/cp-command-linux-examples/)\n",
        "\n",
        "\n",
        "Tips\n",
        "- dot in terminal: (1) current directory; (2) temporary variable for storing all data; \n",
        "- check version of pip via: !python -m pip --version\n",
        "- \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwdsBdGhFanc"
      },
      "source": [
        "# install object detection API\n",
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41pSSeizCrx-"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "JYI - Exploration of installed object detection APIs\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-vSoimgCNOo"
      },
      "source": [
        "Tips\n",
        "- use %%bash in the beginning to start shell scripting programming\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuT19NJTO5kG"
      },
      "source": [
        "!pwd\n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2o9P5lsQlSO"
      },
      "source": [
        "Tips\n",
        "- after runing the bash commands, the dirctory will return to the original directory when the bash script is run\n",
        "- \n",
        "\n",
        "ToDos\n",
        "- how to import functions from deep_speech, lstm_object_detection\n",
        "- play with a complete research project in a research paper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JCeQU3fkayh"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Import dependencies\n",
        "- the following is required for importing functions from object_detection.utils\n",
        "\n",
        "%%bash\n",
        "sudo apt install -y protobuf-compiler\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=.\n",
        "cp object_detection/packages/tf2/setup.py .\n",
        "python -m pip install .\n",
        "\n",
        "- \n",
        "\"\"\"\n",
        "# import dependencies\n",
        "from urllib.request import OpenerDirector\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncHasQpZDo4S"
      },
      "source": [
        "Documentations\n",
        "- urllib.request.OpenDirector\n",
        "- object_detection.utils.label_map_util\n",
        "- object_detection.utils.visualization_utils\n",
        "- object_detection.utils.ops\n",
        "\n",
        "ToDos\n",
        "- how to import libraries, module, directory, functions?\n",
        "- \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKtD0IeclbL5"
      },
      "source": [
        "# Step: Load label map data (for plotting).\n",
        "\n",
        "Tops\n",
        "- Label maps correspond index numbers to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine.\n",
        "- load from the repository that we loaded the Object Detection API code\n",
        "- label map is a nested dictionary, outter dictionary contains the index of objects. Inner dictionary contains 'id' and 'name' of objects\n",
        "- try different label maps to see how data is stored"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mucYUS6exUJ"
      },
      "source": [
        "PATH_TO_LABELS = './models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N76-zmXpSYQo"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of other label_map\n",
        "\"\"\"\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/face_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # face\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/ava_label_map_v2.1.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # activities\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/face_person_with_keypoints_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # key points in a face\n",
        "# print(f\"cate_index[1]: {cate_index[1]}\")\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/fgvc_2854_classes_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") #?\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/kitti_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # car & ped\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/mscoco_complete_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # various objects\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/oid_bbox_trainable_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # various objects\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/pascal_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # various objects\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/pet_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\") # different pets\n",
        "\n",
        "# path_to_label = './models/research/object_detection/data/snapshot_serengeti_label_map.pbtxt'\n",
        "# cate_index = label_map_util.create_category_index_from_labelmap(path_to_label,\n",
        "#                                                                 use_display_name=True)\n",
        "# print(f\"cate_index: {cate_index}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zk_xb5fEU5m"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of label map\n",
        "\"\"\"\n",
        "\n",
        "# print(f\"category_index: {category_index}\")\n",
        "# print(f\"category_index[2]: {category_index[2]}\")\n",
        "# print(f\"category_index[2]['id']: {category_index[2]['id']}\")\n",
        "# print(f\"category_index[2]['name']: {category_index[2]['name']}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6917xnUSlp9x"
      },
      "source": [
        "# Step: Build a detection model and load pre-trained model weights\n",
        "\n",
        "Tips\n",
        "- choose which Object Detection model we will use.\n",
        "Select the architecture and it will be loaded automatically.\n",
        "If you want to change the model to try other architectures later, just change the next cell and execute following ones.\n",
        "- we specify the http of the model in model_handle\n",
        "\n",
        "\n",
        "ToDos\n",
        "- construct customized object detection models\n",
        "- retrain the model\n",
        "- fine-tuning of models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtwrSqvakTNn"
      },
      "source": [
        "# Select model and specify the web link of models\n",
        "model_display_name = 'CenterNet HourGlass104 Keypoints 512x512' \n",
        "model_handle = ALL_MODELS[model_display_name]\n",
        "\n",
        "print('Selected model:'+ model_display_name)\n",
        "print('Model Handle at TensorFlow Hub: {}'.format(model_handle))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Xn6SofEu04"
      },
      "source": [
        "Tips\n",
        "- create code chunk title by using #@title\n",
        "- list of all models: ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muhUt-wWL582"
      },
      "source": [
        "# Step: Loading the selected model from TensorFlow Hub\n",
        "\n",
        "Tips\n",
        "- Here we just need the model handle that was selected and use the Tensorflow Hub library to load it to memory.\n",
        "- handle is essentially the hyperlink to the model\n",
        "- \n",
        "\n",
        "Documentations\n",
        "- hub.load, https://github.com/tensorflow/hub/blob/v0.12.0/tensorflow_hub/module_v2.py#L50-L108\n",
        "- tf.saved_model.load, https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/saved_model/load.py#L778-L869\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBuD07fLlcEO"
      },
      "source": [
        "# Load model\n",
        "print('loading model...')\n",
        "hub_model = hub.load(model_handle)\n",
        "print('model loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3-bvme4Fczz"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of loaded model\n",
        "- hub_model has the following attributes or methods: \n",
        "hub_model.graph_debug_info,hub_model.tensorflow_git_version,\n",
        "hub_model.tensorflow_version,hub_model.signature \n",
        "\"\"\"\n",
        "\n",
        "print(f\"hub_model: {hub_model}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m94XUEtFWSCU"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of loaded model\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQpUOgWsWVkX"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of loading other models\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlGpyG10fQPg"
      },
      "source": [
        "ToDos\n",
        "- how to explore and use loaded models? \n",
        "\n",
        "Tips\n",
        "- tensorflow_hub.load returns a trackable object\n",
        "\n",
        "Documentations\n",
        "- hub.load, similar to tf.saved_model.load, https://www.tensorflow.org/hub/api_docs/python/hub/load\n",
        "- tf.saved_model.load, https://www.tensorflow.org/api_docs/python/tf/saved_model/load\n",
        "\n",
        "- exploration of loaded model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIawRDKPPnd4"
      },
      "source": [
        "# Step: Loading an image\n",
        "\n",
        "ToDos\n",
        "- try difference images, ['Beach', 'Dogs', 'Naxos Taverna', 'Beatles', 'Phones', 'Birds']\n",
        "- Try running inference on your own images, just upload them to colab and load the same way it's done in the cell below.\n",
        "- Modify some of the input images and see if detection still works.  Some simple things to try out here include flipping the image horizontally, or converting to grayscale (note that we still expect the input image to have 3 channels).\n",
        "- when using images with an alpha channel, the model expect 3 channels images and the alpha will count as a 4th.\n",
        "- alpha channel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJN5dt7RgscC"
      },
      "source": [
        "# utility functions\n",
        "\n",
        "# Flip horizontally\n",
        "def horizontal_flip(image_np):\n",
        "  \"\"\"\n",
        "  image_np: a single image stored in numpy array\n",
        "  \"\"\"\n",
        "  image_np = np.fliplr(image_np).copy()\n",
        "  return image_np\n",
        "\n",
        "# Flip vertical\n",
        "\n",
        "def vertical_flip(image_np):\n",
        "\n",
        "  pass\n",
        "\n",
        "# Convert image to grayscale\n",
        "def to_gray(image_np):\n",
        "  \"\"\"\n",
        "  image_np: a single image stored in numpy array\n",
        "\n",
        "  tips\n",
        "  - average over channels, then replicate the channel by three times\n",
        "  - the returned image is still of shape (W,H,C)\n",
        "  \"\"\"\n",
        "  image_np = np.tile(\n",
        "      np.mean(image_np, 2, keepdims=True),\n",
        "      (1, 1, 3)).astype(np.uint8)\n",
        "  return image_np\n",
        "\n",
        "# pixel shifting\n",
        "\n",
        "def shift(image_np):\n",
        "  pass\n",
        "\n",
        "# rotation\n",
        "\n",
        "def rotation(image_np):\n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYO3t18JhQ6M"
      },
      "source": [
        "ToDos\n",
        "- implement vertical_flip, shift, rotation\n",
        "\n",
        "Documentations\n",
        "- np.fliplr, https://numpy.org/doc/stable/reference/generated/numpy.fliplr.html\n",
        "- np.tile, replicate given number of times along given dimension, https://numpy.org/doc/stable/reference/generated/numpy.tile.html\n",
        "- np.mean, https://numpy.org/doc/stable/reference/generated/numpy.mean.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-AWUQ1wIEr"
      },
      "source": [
        "# image exploration\n",
        "\n",
        "IMAGES_FOR_TEST = {\n",
        "  'Beach' : 'models/research/object_detection/test_images/image2.jpg',\n",
        "  'Dogs' : 'models/research/object_detection/test_images/image1.jpg',\n",
        "  # By Heiko Gorski, Source: https://commons.wikimedia.org/wiki/File:Naxos_Taverna.jpg\n",
        "  'Naxos Taverna' : 'https://upload.wikimedia.org/wikipedia/commons/6/60/Naxos_Taverna.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_Coleoptera_of_the_British_islands_(Plate_125)_(8592917784).jpg\n",
        "  'Beatles' : 'https://upload.wikimedia.org/wikipedia/commons/1/1b/The_Coleoptera_of_the_British_islands_%28Plate_125%29_%288592917784%29.jpg',\n",
        "  # By Am√©rico Toledano, Source: https://commons.wikimedia.org/wiki/File:Biblioteca_Maim%C3%B3nides,_Campus_Universitario_de_Rabanales_007.jpg\n",
        "  'Phones' : 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg/1024px-Biblioteca_Maim%C3%B3nides%2C_Campus_Universitario_de_Rabanales_007.jpg',\n",
        "  # Source: https://commons.wikimedia.org/wiki/File:The_smaller_British_birds_(8053836633).jpg\n",
        "  'Birds' : 'https://upload.wikimedia.org/wikipedia/commons/0/09/The_smaller_British_birds_%288053836633%29.jpg',\n",
        "}\n",
        "\n",
        "selected_image = 'Beach' \n",
        "flip_image_horizontally = False \n",
        "convert_image_to_grayscale = False \n",
        "\n",
        "image_path = IMAGES_FOR_TEST[selected_image]\n",
        "image_np = load_image_into_numpy_array(image_path)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(image_np[0])\n",
        "plt.show()\n",
        "\n",
        "# image_fliplr = horizontal_flip(image_np[0])\n",
        "# plt.figure(figsize=(10,10))\n",
        "# plt.imshow(image_fliplr)\n",
        "# plt.show()\n",
        "\n",
        "# image_gray = to_gray(image_np[0])\n",
        "# plt.figure(figsize=(10,10))\n",
        "# plt.imshow(image_gray)\n",
        "# plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTHsFjR6HNwb"
      },
      "source": [
        "# Step: Doing the inference\n",
        "\n",
        "Tips\n",
        "- Print out `result['detection_boxes']` and try to match the box locations to the boxes in the image.  Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\n",
        "- representation of detection boxes\n",
        "- \n",
        "- inspect other output keys present in the result. A full documentation can be seen on the models documentation page (pointing your browser to the model handle printed earlier)\n",
        "\n",
        "ToDos\n",
        "- retrain models\n",
        "- transfer learning\n",
        "- customized models\n",
        "- exploration of loaded model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gb_siXKcnnGC"
      },
      "source": [
        "# running inference\n",
        "results = hub_model(image_np)\n",
        "\n",
        "# different object detection models have additional results\n",
        "# all of them are explained in the documentation\n",
        "result = {key:value.numpy() for key,value in results.items()}\n",
        "print(result.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "St-85zMHiFEV"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of model\n",
        "\"\"\"\n",
        "\n",
        "print(hub_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwUYzC0og2_h"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Exploration of results\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# print(f\"results: {type(results)}\")\n",
        "# #print(f\"results: {results['detection_boxes']}\")\n",
        "# print(f\"results: {type(results['detection_boxes'])}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSzuB5X7lcyV"
      },
      "source": [
        "ToDos\n",
        "- meaning of result['detection_scores']\n",
        "- meaning of result['detection_keypoints']\n",
        "- difference between EagerTensor, Tensor, numpy.ndarry?\n",
        "\n",
        "Tips\n",
        "- the category_index is not for all the test images. Each test image may have its own category index set\n",
        "- tensorflow eager execution computes results immediately, more see [ref 1](https://www.tensorflow.org/guide/eager)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD9EjIhikAVo"
      },
      "source": [
        "\"\"\"\n",
        "JYI - Inference results exploration\n",
        "- \n",
        "\"\"\"\n",
        "\n",
        "# # detection_scores\n",
        "# print(f\"result['detection_scores']: {result['detection_scores']}\")\n",
        "# print(f\"result['detection_scores'] sum: {np.sum(result['detection_scores'])}\")\n",
        "# plt.figure()\n",
        "# plt.plot(result['detection_scores'][0]) # (100,1)\n",
        "# plt.show()\n",
        "\n",
        "# # detection_keypoints\n",
        "# print(f\"result['detection_keypoints']: {result['detection_keypoints']}\")\n",
        "# print(f\"result['detection_keypoints shape']: {result['detection_keypoints'].shape}\") # (1,100,17,2)\n",
        "# print(f\"result['detection_keypoints range']: {np.max(result['detection_keypoints'])}, {np.min(result['detection_keypoints'])}\") # (1,100,17,2)\n",
        "# plt.figure(figsize=(24,24))\n",
        "# for row_ind in range(10):\n",
        "#   for col_ind in range(10):\n",
        "\n",
        "#     temp_ind = row_ind*10 + col_ind\n",
        "#     plt.subplot(10,10,temp_ind+1)\n",
        "    \n",
        "#     plt.plot(result['detection_keypoints'][0,temp_ind][:,0],\n",
        "#              result['detection_keypoints'][0,temp_ind][:,1],'+')\n",
        "# plt.show()\n",
        "\n",
        "# # temp_ind =1 \n",
        "# plt.figure()\n",
        "# plt.plot(result['detection_keypoints'][0,1][:,0],\n",
        "#          result['detection_keypoints'][0,1][:,1],\"+\")\n",
        "# plt.show()\n",
        "\n",
        "# # detection classes\n",
        "# print(f\"result['detection_classes']: {result['detection_classes']}\")\n",
        "# print(f\"result['detection_classes']: {result['detection_classes'].shape}\")\n",
        "# print(f\"category_index: {category_index[result['detection_classes'][0,1]]}\")\n",
        "# print(f\"category_index: {category_index[38]}\")\n",
        "\n",
        "# # num_detections\n",
        "# # print(f\"result['num_detections']: {result['num_detections']}\")\n",
        "\n",
        "# # detection_keypoint_scores\n",
        "# print(f\"result['detection_keypoint_scores']: {result['detection_keypoint_scores']}\")\n",
        "# print(f\"result['detection_keypoint_scores shape']: {result['detection_keypoint_scores'].shape}\")\n",
        "# plt.figure()\n",
        "# plt.subplot(2,1,1)\n",
        "# plt.plot(result['detection_keypoint_scores'][0,1])\n",
        "# print(f\"sum result['detection_keypoint_scores'][0,1]: {np.sum(result['detection_keypoint_scores'][0,1])}\")\n",
        "# plt.subplot(2,1,2)\n",
        "# plt.plot(result['detection_keypoint_scores'][0,:,1])\n",
        "# print(f\"sum result['result['detection_keypoint_scores'][0,:,1]: {np.sum(result['detection_keypoint_scores'][0,:,1])}\")\n",
        "# plt.show()\n",
        "\n",
        "# # detection_boxes\n",
        "# print(f\"result['detection_boxes']: {result['detection_boxes']}\")\n",
        "# print(f\"result['detection_boxes shape']: {result['detection_boxes'].shape}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9BjqGusu9v1"
      },
      "source": [
        "Tips\n",
        "- num_detections: a tf.int tensor with only one value, the number of detections [N].\n",
        "- detection_boxes: a tf.float32 tensor of shape [N, 4] containing bounding box coordinates in the following order: [ymin, xmin, ymax, xmax]. each object has a box\n",
        "- detection_classes: a tf.int tensor of shape [N] containing detection class index from the label file. numerical class index of objects\n",
        "- detection_scores: a tf.float32 tensor of shape [N] containing detection scores. confidence score for classifying the object as a particular class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5VYaBoeeFM"
      },
      "source": [
        "# Step: Visualizing the results\n",
        "\n",
        "Tips\n",
        "- we will need the TensorFlow Object Detection API to show the squares from the inference step (and the keypoints when available). [here](https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py)\n",
        "- you can, for example, set `min_score_thresh` to other values (between 0 and 1) to allow more detections in or to filter out more detections.\n",
        "\n",
        "ToDos\n",
        "- \n",
        "\n",
        "Documentations\n",
        "- copy\n",
        "- deepcopy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdxBIXWNxt97"
      },
      "source": [
        "Tips\n",
        "- in viz_utils.visualize_boxes_and_labels_on_image_array, the image input should not contain batch number dimension, and only a single image should be fed. Similarly, for detection_keypoints, detection_keypoints_scores, detection_boxes, detection_classes, detection_scores\n",
        "- viz_utils: from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "ToDos\n",
        "- meaning of COCO17_HUMAN_POSE_KEYPOINTS\n",
        "- what's return of viz_utils.visualize_boxes_and_labels_on_image_array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O7rV8g9s8Bz"
      },
      "source": [
        "label_id_offset = 0\n",
        "image_np_with_detections = image_np.copy()\n",
        "\n",
        "# show image without boxes or labels or scores\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()\n",
        "\n",
        "# Use keypoints if available in detections\n",
        "keypoints, keypoint_scores = None, None\n",
        "if 'detection_keypoints' in result:\n",
        "  keypoints = result['detection_keypoints'][0] # (100,17,2)\n",
        "  keypoint_scores = result['detection_keypoint_scores'][0] # (100,17)\n",
        "\n",
        "# show image with boxes, labels, and scores\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_detections[0],\n",
        "      result['detection_boxes'][0],\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=.30,\n",
        "      agnostic_mode=False,\n",
        "      keypoints=keypoints,\n",
        "      keypoint_scores=keypoint_scores,\n",
        "      keypoint_edges=COCO17_HUMAN_POSE_KEYPOINTS)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np_with_detections[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaw6Xi08NpEP"
      },
      "source": [
        "# Step: Segmentation\n",
        "\n",
        "Tips\n",
        "- Among the available object detection models there's Mask R-CNN and the output of this model allows instance segmentation. To visualize it we will use the same method we did before but adding an aditional parameter: `instance_masks=output_dict.get('detection_masks_reframed', None)`\n",
        "\n",
        "ToDos\n",
        "- difference between object detection and segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mV-y_UNzbcF"
      },
      "source": [
        "Documentations\n",
        "- tf.convert_to_tensor\n",
        "- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zl3qdtR1OvM_"
      },
      "source": [
        "# Handle models with masks:\n",
        "image_np_with_mask = image_np.copy()\n",
        "\n",
        "if 'detection_masks' in result:\n",
        "  # we need to convert np.arrays to tensors\n",
        "  detection_masks = tf.convert_to_tensor(result['detection_masks'][0])\n",
        "  detection_boxes = tf.convert_to_tensor(result['detection_boxes'][0])\n",
        "\n",
        "  # Reframe the the bbox mask to the image size.\n",
        "  detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "            detection_masks, detection_boxes,\n",
        "              image_np.shape[1], image_np.shape[2])\n",
        "  detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                      tf.uint8)\n",
        "  result['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np_with_mask[0],\n",
        "      result['detection_boxes'][0],\n",
        "      (result['detection_classes'][0] + label_id_offset).astype(int),\n",
        "      result['detection_scores'][0],\n",
        "      category_index,\n",
        "      use_normalized_coordinates=True,\n",
        "      max_boxes_to_draw=200,\n",
        "      min_score_thresh=.30,\n",
        "      agnostic_mode=False,\n",
        "      instance_masks=result.get('detection_masks_reframed', None),\n",
        "      line_thickness=8)\n",
        "\n",
        "plt.figure(figsize=(24,32))\n",
        "plt.imshow(image_np_with_mask[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}